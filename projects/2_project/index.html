<!DOCTYPE html>
<!-- _layouts/distill.html -->
<html>
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Chi-Hsuan  Chang | Meta-Learning for Question Answering on SQuAD 2.0</title>
    <meta name="author" content="Chi-Hsuan  Chang" />
    <meta name="description" content="This post was summarized from my final project @Stanford CS224N." />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://achchg.github.io/projects/2_project/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    


    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Meta-Learning for Question Answering on SQuAD 2.0",
      "description": "This post was summarized from my final project @Stanford CS224N.",
      "published": "March 14, 2022",
      "authors": [
        {
          "author": "Chi-Hsuan Chang",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body class="fixed-top-nav">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://achchg.github.io/"><span class="font-weight-bold">Chi-Hsuan</span>   Chang</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>Meta-Learning for Question Answering on SQuAD 2.0</h1>
        <p>This post was summarized from my final project @Stanford CS224N.</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        

        <p>In a Question Answering (QA) system, the system learn to answer a question by properly understanding an associated paragraph. However, developing a QA system that performs robustly well across all domains can be extra challenging as we do not always have abundant amount of data across domains. Therefore, one area of focus in this field has been learning to train a model to learn new task with limited data available (e.g. Few-Shot Learning, FSL).</p>

<p>Meta-learning in supervised learning, in particular, has been known to perform well in FSL, with the concept being teaching the models learn to set up initial parameters well that enable the model to learn a new task after seeing a few samples of the associated data.<d-cite key="finn2017, abs-1904-05046"></d-cite> In this study, we were given a large amount of in-domain (IND) samples with only limited samples of out-of-domain (OOD) set. We were provided with a fine-tuned (FT) DistilBERT model <d-cite key="sanh2020distilbert"></d-cite> that knew to perform well on the IND set. To improve the robustness of the FT baseline model performance on OOD set, we trained:</p>

<ul>
  <li><strong>MAML models from scratch</strong></li>
  <li><strong>MAML models after baseline model was pre-trained and fine-tuned</strong></li>
</ul>

<h3 id="background">Background</h3>

<p><strong><a href="https://rajpurkar.github.io/SQuAD-explorer/explore/v2.0/dev/" target="_blank" rel="noopener noreferrer">SQuAD 2.0 dataset.</a></strong>
Three in‐domain (SQuAD, NewsQA, Natural Questions) and three out‐of‐domain (DuoRC, RACE, RelationExtraction) datasets. The in‐domain (IND) and out‐of‐domain (OOD) datasets contain 50K and 127 question‐passage‐answer samples each.</p>

<p><strong>Model‐Agnostic Meta‐Learning (MAML).</strong>
MAML was originally proposed by Finn et al 2017 <d-cite key="finn2017"></d-cite> to train the models their own initial parameters so that the parameters allow the algorithm to perform well on a new task (”learn‐to‐learn”) after one or a few gradient steps of updates with few‐shot data availability.</p>

<h3 id="methods">Methods</h3>

<p><strong>Fine-tuned Baseline.</strong> A fine‐tuned (FT) pre‐trained transformer model ‐ DistilBERT.<d-cite key="sanh2020distilbert"></d-cite> The baseline QA model was trained on the overall IND training set, and was validated on the IND validation set.</p>

<p><strong>MAML DistilBERT.</strong> We adapted MAML<d-cite key="finn2017"></d-cite> as a framework to train our robust QA system that performs well across different domains.</p>
<ul>
  <li>
    <p>We defined the baseline DistilBERT <d-cite key="sanh2020distilbert"></d-cite>  as our base learner ($f_{\theta}$)</p>
  </li>
  <li>
    <p>We implemented a task method rather than to pre-define a K-shot task pool ($p(\mathcal{T})$). As K sample support ($\mathcal{D}_i$) and query ($\mathcal{D}_i$’ ) sets can come from IND and OOD training datasets in different experiments</p>
  </li>
  <li>
    <p>We used the same loss function ($\mathcal{L}$, $\textbf{loss} = - \log p_{start}(i) - \log p_{end}(j)$) as the baseline</p>
  </li>
</ul>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/mamlBert_illustration-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/mamlBert_illustration-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/mamlBert_illustration-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/mamlBert_illustration.png" title="example image">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 1. Model architecture of MAML DistilBERT. Training support and query sets can come from In‐domain or OOD datasets and are a factor we experimented on.
</div>

<p><strong>FT Baseline + MAML DistilBERT.</strong> In addition to training MAML model from scratch, we leveraged the FT DistillBERT (Baseline) model and trained the MAML models from the FT checkpoint.</p>

<h3 id="experiments">Experiments</h3>
<p>If not otherwise specified, batch size for all experiments were 16. To avoid GPU out-of-memory issue, data was loaded in either batch size of 1 or 4 to accumulate the loss. Model is updated at batch size of 16.</p>

<p><strong>Experiment #1: MAML DistilBERT without FT Baseline</strong></p>
<ul>
  <li>
<strong>K-shot:</strong> MAML-20-d vs. MAML-2000-d</li>
  <li>
<strong>learning rate:</strong> MAML-20-a vs. MAML-20-b vs. MAML-20-d</li>
  <li>
<strong>domain variability in training support:</strong> MAML-20-b vs. MAML-20-c</li>
</ul>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/table1-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/table1-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/table1-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/table1.jpg" title="example image">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Table 1. Experiment 1: Model configuration.
</div>

<p><strong>Experiment #2: Training MAML after FT Baseline</strong></p>
<ul>
  <li>
<strong>K-shot:</strong> M1/2/4 vs. M3, M7 vs. M8, M9 vs. M10</li>
  <li>
<strong>IND or OOD for MAML training:</strong> M1 vs. M6 vs. M7 vs. M10, M2 vs. M5 vs. M7 vs. M10</li>
  <li>
<strong>training time:</strong> M1 vs. M2 vs. M4, M5 vs. M5</li>
</ul>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/table2-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/table2-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/table2-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/table2.png" title="example image">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Table 2. Experiment 1: Model configuration.
</div>

<h3 id="analysis">Analysis</h3>
<p><strong>Key-takeaway #1: MAML DistilBERT without FT Baseline couldn’t achieve the same level of model performance as the FT Baseline.</strong></p>

<ul>
  <li>This can be because of the large IND data available during baseline model pre-training/fine-tuning.</li>
  <li>Larger learning rate helped in faster adaptation with the MAML model given the same sample size as it  allowed more aggressive exploration in the gradient at the beginning.</li>
  <li>Larger domain variability in support/query reached similar F1 performance but lower EM performance. This was intuitive as the MAML was learning to learn and exposed to a lot of topics as few-shot learning though benefit understanding synergies across domains, the model also became more “general” and “robust”.</li>
</ul>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/model1-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/model1-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/model1-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/model1.png" title="example image">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 2. Experiment #1 model descending sorted by EM (OOD eval)
</div>

<p><strong>Key-takeaway #2: Training MAML after FT Baseline outperformed FT Baseline occasionally. More experimentation configurations in learning rate and domain variability could be explored.</strong></p>

<ul>
  <li>
<strong>M2</strong>, a 10-task 20-shot MAML training on OOD samples post pre-training outperformed the FT Baseline in OOD validation set by <strong>1.22%</strong> in F1 and <strong>3.04%</strong> in EM. Its performance in IND validation set dropped by <strong>4.57%</strong> in F1 and <strong>6.49%</strong> in EM. This showed the scarification of model performance on the IND datasets in gaining additional robustness on an OOD dataset.</li>
  <li>
<strong>M8</strong>, a 10-task 200-shot MAML training on IND samples post pre-training outperformed the FT Baseline in OOD validation set by <strong>0.44%</strong> in F1 and <strong>0%</strong> in EM, and in IND validation set by <strong>0.78%</strong> in F1 and <strong>1.10%</strong> in EM. This showed that continuously training with the same domain datasets with MAML contributed less improvements than training with few OOD samples.</li>
</ul>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/model2-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/model2-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/model2-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/model2.png" title="example image">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Figure 3. Experiment #2 model descending sorted by EM (OOD eval)
</div>

<h3 id="conclusions">Conclusions</h3>

<p>MAML was a good‐to‐explore to achieve cross‐domain model robustness. MAML might not be the best framework in context of a large amount IND set and small amount OOD set. Training MAML post baseline model pre‐training and fine‐tuning performed occasionally better than the FT baseline model likely due to additional OOD tasks used to learn by the MAML model.</p>

<p>Full copy of the paper could be found <a href="https://web.stanford.edu/class/cs224n/reports/default_116613241.pdf" target="_blank" rel="noopener noreferrer">here</a>.</p>

<p>Github of this project could be found <a href="https://github.com/achchg/cs224_robustqa" target="_blank" rel="noopener noreferrer">here</a>.</p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Chi-Hsuan  Chang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

      </div>
    </footer>

  </body>

  <d-bibliography src="/assets/bibliography/maml.bib">
  </d-bibliography>

  <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

  <script src="/assets/js/distillpub/overrides.js"></script>

</html>
