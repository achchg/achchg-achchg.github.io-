<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Chi-Hsuan  Chang | Adam (adaptive moment estimation) optimizer</title>
    <meta name="author" content="Chi-Hsuan  Chang" />
    <meta name="description" content="Day 12" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://achchg.github.io/blog/2022/Adam_optimizer/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://achchg.github.io/"><span class="font-weight-bold">Chi-Hsuan</span>   Chang</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Adam (adaptive moment estimation) optimizer</h1>
    <p class="post-meta">October 4, 2022</p>
    <p class="post-tags">
      <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>
        ·  
        <a href="/blog/tag/review">
          <i class="fas fa-hashtag fa-sm"></i> review</a>  
          
        ·  
        <a href="/blog/category/optimization">
          <i class="fas fa-tag fa-sm"></i> optimization</a>  
          <a href="/blog/category/gradient">
          <i class="fas fa-tag fa-sm"></i> gradient</a>  
          

    </p>
  </header>

  <article class="post-content">
    <p>Reviewing <a href="https://web.stanford.edu/class/cs224n/assignments/a3_handout.pdf" target="_blank" rel="noopener noreferrer">week 3 assignment</a> of NLP with deep learning brought Adam optimization algorithm back to my attention. Here I’d summarize what I did in my homework 3 for my future reference:</p>

<h4 id="adam-optimizer">Adam optimizer</h4>
<p>From standard SGD, we would use a mini-batch (e.g. single sample) of data in the update rule below for updating the \(J(\theta)\):</p>

\[\theta := \theta - \alpha \nabla_\theta J(\theta)\]

<p>where \(\alpha\) is the learning rate and \(\nabla_\theta J(\theta)\) represent the partial derivatives of the cost function wrt \(\theta\).</p>

<p>Adam optimization, in addition, takes 2 additional steps beyond SGD:</p>

<h5 id="update-biased-first-order-moment-estimate">Update biased first order moment estimate</h5>

\[\begin{align*}
m &amp; := \beta_1 m + (1 − \beta_1)\nabla_\theta J(\theta)\\
\theta &amp; := \theta - \alpha m
\end{align*}\]

<p>As \(m\) is set as a weighted average of the rolling gradient average of the <strong>previous</strong> iterations and the gradient of the <strong>current</strong> iteration, we can expect the <strong>momentum</strong> step making the gradient descent update smoother than that of the SGD (which only consider the <strong>current</strong> iteration). The current gradient (\(\nabla_{\theta} J(\theta)\)) will be weighted larger than the individual gradients after k (\(\frac{\beta_1}{1-\beta_1}\)) iterations, as the individual previous gradients has a weight of \(\frac{\beta_1}{k}\) (where k = num of past iterations).</p>

<h5 id="update-biased-second-order-raw-moment-estimate">Update biased second order raw moment estimate</h5>

\[\begin{align*}
v &amp; := \beta_2 v + (1 − \beta_2)(\nabla_\theta J(\theta) \odot \nabla_\theta J(\theta))\\
\theta &amp; := \theta - \alpha \frac{m}{\sqrt{v}}
\end{align*}\]

<p>As \(m\) is divided by the \(\sqrt{v}\) (the gradient of Adam), the gradients that have smaller gradient magnitude (\(v\)) will get larger updates. As \(v\) is derived from squared of \(\nabla_{\theta} J(\theta)\), \(v\) is usually associated with smaller gradient. This might help further smoothing out the gradient descent from the <strong>momentum</strong> step, by giving larger weights to the smaller gradients when the SGD update does not guarantee continuous descending in the gradients.</p>

<h5 id="note-method-of-moment">Note: Method of moment</h5>
<p><a href="https://en.wikipedia.org/wiki/Method_of_moments_(statistics)" target="_blank" rel="noopener noreferrer">Method of moment</a> in statistics implies the following:</p>

<p>The \(k^{th}\) moment of a random variable X with its pdf, \(f(x)\) can be expressed as:</p>

\[E(X^k) = \int_X x^k f(x) dx\]

<p>Therefore, the first moment of X is \(E(X)\), which is the mean of the distribution; and the second moment of X is \(E(X^2)\), which is the sum of mean squared and the variance (\(\text{Var}(X) = E(X^2) - E(X)^2\)).
<!-- 
Example notebook with above example can be found [here](https://github.com/achchg/achchg.github.io/blob/master/jupyternb/2022-09-29-Stochastic_gradient_descent.ipynb). --></p>

<p>Original Adam paper is <a href="https://arxiv.org/pdf/1412.6980.pdf" target="_blank" rel="noopener noreferrer">here</a>; Helpful <a href="https://gregorygundersen.com/blog/2020/04/11/moments/" target="_blank" rel="noopener noreferrer">documentation</a> of moment statistics.</p>

  </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Chi-Hsuan  Chang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

