---
layout: post
title: RNN/LSTM
date: 2022-10-07
description: Day 15
tags: review
categories: dl sequence
---

My notes from reviewing RNN model in NLP, following the flow of [RNN](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf) and [Seq2seq](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes06-NMT_seq2seq_attention.pdf) lecture notes.

#### RNN architecture

I'll use the below figure cited from CS224n lecture 5-6 notes of RNN to summarize RNN:

"The elements (e.g. words) were fed into the algorithm one after one along (e.g. the $$t^{th}$$ element of the input sequence, $$w_{t}$$) with the hidden output layer ($$h_{t-1}$$) from the previous timestamp ($$t-1$$) in predicting the most likely next element of the output sequence ($$y_{t}$$)" 

$$
\begin{align*}
h_t & = \sigma(W^{(hh)}h_{t−1} + W^{(hx)}x_{t})\\
y_t & = \text{softmax}(W^{(S)}h_t)
\end{align*}
$$


<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/rnn.png" title="example image" %}
    </div>
</div>

- Parameters to be solved: $$h_t$$, $$W^{(hh)}$$, $$W^{(hx)}$$ and $$W^{(S)}$$
- With the softmax activation function, the common loss function for RNN is the cross-entropy loss (as derived previously in the [SGD post with logistic regression](https://achchg.github.io/blog/2022/Stochastic_gradient_descent/)):


$$
\begin{align*}
J^{(t)}(\theta) &= −\Sigma_{j=1}^{|V|}y_{t,j} * log(\hat{y}_{t,j}) \\
J(\theta) &= \frac{1}{T}\Sigma_{t=1}^TJ^{(t)}(\theta)
\end{align*}
$$

- Evaluation method of RNN model: **Perplexity!** The lower the perplexity, the better the model.

$$
\begin{align*}
\text{perplexity} = \exp({J})
\end{align*}
$$


#### Sequence-to-sequence (Seq2seq) model




Nice RNN resources:
- [CS224N](https://web.stanford.edu/class/cs224n/slides/cs224n-2022-lecture05-rnnlm.pdf)
- [StatQuest](https://www.youtube.com/watch?v=AsNTP8Kwu80)